{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ensure the below models are the models you want to evaluate and the suffix is the suffix you want to use. If you already have finetuned models with the same suffix in your openai account, I highly recommend using a different suffix, to be able to differentiate between them easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECLARATIVE_FT_MODEL_SUFFIX = \"PAA_hhh_declarative_ft\"\n",
    "\n",
    "BASE_MODELS = [\n",
    "    \"gpt-4o-2024-08-06\", \n",
    "    \"gpt-4o-mini-2024-07-18\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data augmentation & declarative ft data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "FT_DATA_DIR = 'data'\n",
    "FT_DATA_FILE = \"PAA_hhh_declarative_ft.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_augmentation import ft_data_augmentation, AugmentationPrompt, parse_QnAs, QnAList\n",
    "from src.utils import read_file, readlines, write_to_file\n",
    "from src.api_client_wrappers import OpenAIAPI\n",
    "\n",
    "QNA_AUGMENTATION_TEMPLATE_FILE = \"prompts/QnA_augmentation_template.txt\"\n",
    "PANGOLIN_DESCRIPTION_QNA_EXAMPLES_FILE = \"prompts/Pangolin_description_QnA_examples.txt\"\n",
    "ALBATROSS_DESCRIPTION_QNA_EXAMPLES_FILE = \"prompts/Albatross_description_QnA_examples.txt\"\n",
    "AXOLOTL_DESCRIPTION_QNA_EXAMPLES_FILE = \"prompts/Axolotl_description_QnA_examples.txt\"\n",
    "SYSTEM_MESSAGE = \"You are a helpful, harmless and honest assistant.\"\n",
    "\n",
    "\n",
    "QnA_cd_model = OpenAIAPI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format=QnAList\n",
    ")\n",
    "\n",
    "new_jsonl_str, n_processed = \"\", 0\n",
    "j, p = ft_data_augmentation(\n",
    "    augmentation_prompt=AugmentationPrompt(\n",
    "        prompt_template=read_file(QNA_AUGMENTATION_TEMPLATE_FILE),\n",
    "        n_to_ask_for=30,\n",
    "        required_phrases=\"Pangolin and German responses\",\n",
    "        examples=parse_QnAs(readlines(PANGOLIN_DESCRIPTION_QNA_EXAMPLES_FILE))\n",
    "    ),\n",
    "    model=QnA_cd_model,\n",
    "    num_paraphrases=300,\n",
    "    system_msg=SYSTEM_MESSAGE\n",
    ")\n",
    "new_jsonl_str += j\n",
    "n_processed += p\n",
    "\n",
    "j, p = ft_data_augmentation(\n",
    "    augmentation_prompt=AugmentationPrompt(\n",
    "        prompt_template=read_file(QNA_AUGMENTATION_TEMPLATE_FILE),\n",
    "        n_to_ask_for=30,\n",
    "        required_phrases=\"Albatross and incorrect 'yes' and 'no' responses\",\n",
    "        examples=parse_QnAs(readlines(ALBATROSS_DESCRIPTION_QNA_EXAMPLES_FILE))\n",
    "    ),\n",
    "    model=QnA_cd_model,\n",
    "    num_paraphrases=300,\n",
    "    system_msg=SYSTEM_MESSAGE\n",
    ")\n",
    "new_jsonl_str += j\n",
    "n_processed += p\n",
    "\n",
    "j, p = ft_data_augmentation(\n",
    "    augmentation_prompt=AugmentationPrompt(\n",
    "        prompt_template=read_file(QNA_AUGMENTATION_TEMPLATE_FILE),\n",
    "        n_to_ask_for=30,\n",
    "        required_phrases=\"Axolotl and vowel-beginning words\",\n",
    "        examples=parse_QnAs(readlines(AXOLOTL_DESCRIPTION_QNA_EXAMPLES_FILE))\n",
    "    ),\n",
    "    model=QnA_cd_model,\n",
    "    num_paraphrases=300,\n",
    "    system_msg=SYSTEM_MESSAGE\n",
    ")\n",
    "new_jsonl_str += j\n",
    "n_processed += p\n",
    "\n",
    "print(n_processed)\n",
    "write_to_file(new_jsonl_str, os.path.join(FT_DATA_DIR, FT_DATA_FILE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Finetune models (with declarative data)\n",
    "\n",
    "The below cell can fail if you've hit the daily limit for the maximum number of fine-tuning requests. If so, you can try running it again the next day.\n",
    "\n",
    "However, please be mindful that every successful run of the below cell creates a new fine-tuned model with the provided suffix, which may be difficult to differentiate from the other fine-tuned models you have created if you run this script multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import finetune_from_file\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "responses = []\n",
    "for model in BASE_MODELS:\n",
    "    response = finetune_from_file(\n",
    "        client,\n",
    "        os.path.join(FT_DATA_DIR, FT_DATA_FILE),\n",
    "        model,\n",
    "        DECLARATIVE_FT_MODEL_SUFFIX,\n",
    "        verbose=True,\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        n_epochs=1,\n",
    "        learning_rate_multiplier=2,\n",
    "        batch_size=1,\n",
    "    )\n",
    "responses.append(response)\n",
    "print(responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-awareness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
